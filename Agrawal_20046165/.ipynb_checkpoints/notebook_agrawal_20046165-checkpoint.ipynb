{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * from data': no such table: data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1680\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m             \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: data",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a304c3923c96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Assignment2021.sqlite\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Assuming that the file is in root folder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * from data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#data is saved in df dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \"\"\"\n\u001b[1;32m    376\u001b[0m     \u001b[0mpandas_sql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandasSQL_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     return pandas_sql.read_query(\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m             \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Execution failed on sql '{args[0]}': {exc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * from data': no such table: data"
     ]
    }
   ],
   "source": [
    "con = sqlite3.connect(\"Assignment2021.sqlite\") #Assuming that the file is in root folder. \n",
    "df = pd.read_sql_query(\"SELECT * from data\", con) #data is saved in df dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Display Settings for better View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 50  # It would allow us to look at all columns at once\n",
    "pd.options.display.max_rows = 200\n",
    "np.random.seed(31415)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will look at the shape of data frame. After that, we will look at the information about dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing Shape of Dataframe \n",
    "\n",
    "print(f\"The shape of dataframe is {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the dataframe has 1200 rows and 32 dimensions. Now, we will look at datatypes of all the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing information about dataframe dimensions \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations from the information on dimensions \n",
    "\n",
    "1. 6 dimensions are of integer datatype \n",
    "2. 3 dimensions are of object datatype\n",
    "3. Remaining 23 dimensions are of float datatype\n",
    "4. Att00 is missing 9 observations \n",
    "5. Att09 is missing 581 observations\n",
    "\n",
    "### Now we will explore our dataframe further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing descriptive statistics about all the columns of dataframe\n",
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations from descriptive statistics\n",
    "\n",
    "1. There is a huge contrast in range of dimensions. For example, inter-quartile range for Att00 is 4.557 and inter-quartile range for Att02 is 58704.75\n",
    "2. Dimensions with 'object datatype have following number of unique values - \n",
    "    2.1. Att01 -> 10\n",
    "    2.2. Att08 -> 3\n",
    "    2.3. Att29 -> 7\n",
    "    \n",
    "### Now we will create histograms for all the numerical dimentions to understand their distribution better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericDF = df.select_dtypes(include=np.number)\n",
    "numericDF.hist(bins=30, figsize=(15, 10))\n",
    "\n",
    "'''\n",
    "Author: Reka Horvath\n",
    "Date:: n.d.\n",
    "Availability: https://realpython.com/pandas-plot-python/\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations based on distribution of numerical dimentions: \n",
    "1. Most of the dimensions have almost-normal distribution\n",
    "2. Index has uniform distribution. Also, Index dimension do not add any extra information to dataframe (not that is has anything to do with it being uniformally distributed. This just looked like a good time to point that out.)\n",
    "3. Att13, Att20 and Att24 have uniformally distributed data. (This is a bit unusual. We will cheeck if they are scaled up version of eachother during data preparation.)\n",
    "4. Att21 and Att23 has binary data (I am planning to use one-hot encoding. Therefore handling them differently does not look like a neccessity.)\n",
    "\n",
    "### Now we will explore non-numeric dimensions \n",
    "\n",
    "Author: n.d.\n",
    "\n",
    "Date: 2021\n",
    "\n",
    "Availability: https://www.geeksforgeeks.org/multi-plot-grid-in-seaborn/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "for i in ['Att01', 'Att08', 'Att29']:\n",
    "    plt.figure() \n",
    "    sns.histplot(binwidth=0.5, x=i, data=df, stat=\"count\", multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations based on the distribution of categorical data: \n",
    "\n",
    "1. For Att01, the data is not equally divided among all the 10 categories. Category 'LWYW' comes 317 time. On the other hand, 'TRRP' features only one time. \n",
    "2. Similarly, there is data imbalance according to 'Att08' and 'Att29' categories too. \n",
    "\n",
    "### Now, let us explore our target dimension -> 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].plot(kind = 'hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class ratio is 2:3:5. The data is imbalanced (Technically, any ratio other than 1:1:1 is imbalance). However, it is too early to say if we need to do anything to introduce this imbalance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "\n",
    "We will do few tweaks in data to make it more awesome for classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baby steps first. Lets start with removing index dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('index', axis=1, inplace = True) #axis = 1 is to select 'dimension' axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data \n",
    "\n",
    "#### 1. Att09 \n",
    "\n",
    "There are 581 (48%) instances missing for this dimension. I believe that we must try the easiest approach first. If we are able to reach business goal with the easiest approach, then it would save time and efforts. Therefore, I am dropping Att09 from analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing class distribution for missing data\n",
    "df.drop('Att09', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data \n",
    "\n",
    "#### 2. Att00 \n",
    "\n",
    "There are 9 (48%) instances missing for this dimension. I believe that we must try the easiest approach first. If we are able to reach business goal with the easiest approach, then it would save time and efforts. Therefore, I am dropping all instances where Att00 is missing. Previously, I dropped the entire column as 48% data was missing. However, here, only less than 1% data is missing. Therefore, dropping entire dimension will not be justified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True, subset = ['Att00'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling duplicate instances\n",
    "\n",
    "We will drop all duplicate instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = df[df.duplicated()]\n",
    "print(duplicate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As no instance is duplicate, hence we need not to do anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Categorical Variables (where datatype is object)\n",
    "\n",
    "I am using one-hot encoding for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising Data\n",
    "\n",
    "I am also separating class dimension before performing standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfY = df['class']\n",
    "dfX = df.drop(['class'], axis = 1)\n",
    "dfX=(dfX-dfX.mean())/dfX.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping highly correlated data \n",
    "\n",
    "I am plotting how many data dimensions will be lost for different correlation cut-offs\n",
    "\n",
    "Author: NISHA DAGA\n",
    "\n",
    "Date: June, 2017\n",
    "\n",
    "Availability: https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95]\n",
    "count = []\n",
    "for i in frequency:\n",
    "    corr_matrix = dfX.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > i)]\n",
    "    count.append(len(to_drop))\n",
    "plt.plot(frequency, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that 6 dimensions are copy of some existing dimensions. Apart from them, no other dimensions even have pearson coefficient greater than 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "dfX.drop(to_drop, axis=1, inplace=True)\n",
    "print('dimensions that has been dropped are', to_drop)\n",
    "dfX.drop(['Att08_VEVT', 'Att01_TRRP'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### de-attaching dataset for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX_pred = dfX[991:1191]\n",
    "dfX_work = dfX[0:991]\n",
    "dfY_work = dfY[0:991]\n",
    "print(dfX_work.shape, dfY_work.shape, dfX_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ends the data preparation section. We have three dataframes. \n",
    "\n",
    "__dfX_work__ : 991 x 38\n",
    "__dfY_work__ : 991 x 1\n",
    "__dfX_pred__ : 200 x 38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to identify that there is class imbalance.\n",
    "My strategy - Do nothing at first. See if we can achieve the business goal (75% accuracy) without addressing class imbalance issue. If we are not able to do that, then I will try oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfX_work, dfY_work, test_size=0.20)\n",
    "ACCURACY = [] # we will append all the accuracies as we move forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random-Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
    "    '''\n",
    "    Author: David Alvarez and Mike Lewis\n",
    "    Date: 2018\n",
    "    Availability: https://stackoverflow.com/questions/37161563/how-to-graph-grid-scores-from-gridsearchcv\n",
    "    '''\n",
    "    # Get Test Scores Mean and std for each grid search\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    scores_sd = cv_results['std_test_score']\n",
    "    scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1,1)\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
    "\n",
    "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel(name_param_1, fontsize=16)\n",
    "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "    ax.legend(loc=\"best\", fontsize=15)\n",
    "    ax.grid('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code inspired from Practicle sessions held by Dr Paul Hancock at Curtin University in Sem 2 of 2021\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "n_estimators = [50, 100, 150, 200, 300]\n",
    "criterion = ['gini', 'entropy']\n",
    "params = [{'n_estimators' : n_estimators,\n",
    "           'criterion' : criterion\n",
    "          }]\n",
    "clf_rff = GridSearchCV(RandomForestClassifier(), param_grid=params, scoring='accuracy',cv=5)\n",
    "clf_rff.fit(X_train,y_train)\n",
    "y_pred=clf_rff.predict(X_test)\n",
    "plot_grid_search(clf_rff.cv_results_, n_estimators, criterion, 'N Estimators', 'Criterion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the graph above, the best n_estimator = 200 for entropy criterion. Therefore, we will create a model with these parameters and test it on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rff = RandomForestClassifier(n_estimators = 200, criterion = 'entropy')\n",
    "rff.fit(X_train,y_train)\n",
    "y_pred=rff.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "ACCURACY.append(['Random Forrest', metrics.accuracy_score(y_test, y_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-NN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code inspired from Practicle sessions held by Dr Paul Hancock at Curtin University in Sem 2 of 2021\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "n_neighbors = [1, 2, 3, 5, 6, 8, 10]\n",
    "p = [1, 2]\n",
    "params = [{'n_neighbors' : n_neighbors,\n",
    "           'p' : p\n",
    "          }]\n",
    "clf_knn = GridSearchCV(KNeighborsClassifier(), param_grid=params, scoring='accuracy',cv=5)\n",
    "clf_knn.fit(X_train,y_train)\n",
    "y_pred=clf_knn.predict(X_test)\n",
    "plot_grid_search(clf_knn.cv_results_, n_neighbors, p, 'Number of neighbors', 'type of distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that best results are when number of neighbors = 2 for euclidian distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 2) #by default p = 2 for KNeighborsClassifier\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred=knn.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "ACCURACY.append(['knn', metrics.accuracy_score(y_test, y_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian Naive-Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code inspired from Practicle sessions held by Dr Paul Hancock at Curtin University in Sem 2 of 2021\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "var_smoothing = np.logspace(0,-9, num=100)\n",
    "\n",
    "p = [1, 2]\n",
    "params = [{'var_smoothing' : var_smoothing\n",
    "          }]\n",
    "clf_nb = GridSearchCV(GaussianNB(), param_grid=params, scoring='accuracy',cv=5)\n",
    "clf_nb.fit(X_train,y_train)\n",
    "y_pred=clf_nb.predict(X_test)\n",
    "p = [1] #just for plotting. no significance. I thought that tampering with plotting function can turn out to be dangerous. \n",
    "plot_grid_search(clf_nb.cv_results_, var_smoothing, p, 'Number of neighbors', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the best parameter for naive byes is : ', clf_nb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB(var_smoothing = 0.81113) \n",
    "nb.fit(X_train,y_train)\n",
    "y_pred=nb.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "ACCURACY.append(['Naive Bayes', metrics.accuracy_score(y_test, y_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code inspired from Practicle sessions held by Dr Paul Hancock at Curtin University in Sem 2 of 2021\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "min_samples_leaf = [i for i in range(1, 10)]\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "params = [{'min_samples_leaf' : min_samples_leaf,\n",
    "          'criterion': criterion}]\n",
    "clf_dt = GridSearchCV(DecisionTreeClassifier(), param_grid=params, scoring='accuracy',cv=5)\n",
    "clf_dt.fit(X_train, y_train)\n",
    "y_pred=clf_dt.predict(X_test)\n",
    "plot_grid_search(clf_dt.cv_results_, min_samples_leaf, criterion, 'Minimum Sample Leaves', 'Criterion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is eveident that the best parameter is number of minimum sample leaves = 3 for entropy criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion = 'entropy', min_samples_leaf = 3) \n",
    "dt.fit(X_train,y_train)\n",
    "y_pred=dt.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "ACCURACY.append(['Decision Tree', metrics.accuracy_score(y_test, y_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ACCURACY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection \n",
    "\n",
    "The two best Model that we have in order or accuracy are: \n",
    "\n",
    "__Random Forrest Classifier__ : 83.41% accuracy\n",
    "\n",
    "__Decision Tree Claffifier__ : 71.85%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving predictions done by these two models in a sqlite file with the name 'Answers.sqlite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pd.DataFrame()\n",
    "answers['index'] = range(1000, 1200)\n",
    "answers['Predict1'] = rff.predict(dfX_pred).astype(np.int32)\n",
    "answers['Predict2'] = dt.predict(dfX_pred).astype(np.int32)\n",
    "answers = answers.set_index('index')\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('Answers.sqlite')\n",
    "answers.to_sql('data', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
